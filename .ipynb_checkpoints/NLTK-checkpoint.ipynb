{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk import ConditionalFreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import reuters\n",
    "from urllib import request\n",
    "import re\n",
    "\n",
    "\n",
    "#nltk.download('book')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateWordCounts(text):\n",
    "    # Write your code here\n",
    "    #words = nltk.word_tokenize(text)\n",
    "    n_words=len(text)\n",
    "    print(n_words)\n",
    "    u_words=len(set(text))\n",
    "    print(u_words)\n",
    "    w_avg=int(n_words/u_words)\n",
    "    print(w_avg)\n",
    "\n",
    "def filterWords(text):\n",
    "    # Write your code here\n",
    "    ing_words=[word for word in set(text) if word.isalpha() and word.endswith('ing') ]\n",
    "    large_words=[word for word in set(text) if word.isalpha() and len(word) > 15 ]\n",
    "    upper_words=[word for word in set(text) if word.isupper() and word.isalpha()]\n",
    "    return ing_words, large_words, upper_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "text1_freq = nltk.FreqDist(text6)\n",
    "text1_freq['ARTHUR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16967\n"
     ]
    }
   ],
   "source": [
    "print(len(text6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "wrd = [word for word in text6 if len(word) > 13]\n",
    "print(len(wrd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from nltk.corpus import inaugural\n",
    "    \n",
    "    n_word=len(inaugural.words(fileid))\n",
    "    uni=len(set(inaugural.words(fileid)))\n",
    "    wordcoverage=int(n_word/uni)\n",
    "    ed_words=[word for word in set(inaugural.words(fileid)) if word.endswith('ed') ]\n",
    "    low = [ word.lower() for word in inaugural.words(fileid) ]\n",
    "    text= [word for word in low if word.isalpha()]\n",
    "    textfreq = nltk.FreqDist(text)\n",
    "    wordfreq = textfreq[word]\n",
    "    return wordcoverage, ed_words, wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Indra\\content1.txt C:\\Users\\Indra\\content2.txt\n",
      "C:\\Users\\Indra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    import os.path\n",
    "    from nltk.corpus import PlaintextCorpusReader\n",
    "    path1 = os.path.join(os.getcwd(), \"content1.txt\")\n",
    "    path2=  os.path.join(os.getcwd(), \"content2.txt\")\n",
    "    #os.makedirs(path, exist_ok=True)\n",
    "    print(path1, path2)\n",
    "    f = open(path1, \"w\")\n",
    "    f.write(\"The amber droplet hung from the branch, reaching fullness and ready to drop. It waited.\") \n",
    "    f.close()\n",
    "    f = open(path2, \"w\")\n",
    "    f.write(\"Begin today! That's all the note said. There was no indication from where it came or who may have written it.\")\n",
    "    f.close()\n",
    "    \n",
    "    corpus_root = os.getcwd() #'/usr/share/nltk_data'\n",
    "    print(corpus_root)\n",
    "    text_corpus = PlaintextCorpusReader(corpus_root, '.*')\n",
    "    print(text_corpus.fileids())\n",
    "    #wordlists.fileids()\n",
    "    no_of_corpus1 = len(text_corpus.words('content1.txt'))\n",
    "    no_of_unique_corpus1 =len(set(text_corpus.words('content1.txt')))\n",
    "    no_of_corpus2 = len(text_corpus.words('content2.txt'))\n",
    "    no_of_unique_corpus2 =len(set(text_corpus.words('content2.txt')))\n",
    "    #print(text_corpus, no_of_corpus1, no_of_unique_corpus1, no_of_corpus2, no_of_unique_corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crude', 'nat-gas']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "print(reuters.categories('test/16438'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-9d9ac9847d0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbrown\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m cfd = nltk.ConditionalFreqDist(\n\u001b[1;32m----> 3\u001b[1;33m               word for word in brown.words(categories='news') if word.islower())\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, cond_samples)\u001b[0m\n\u001b[0;32m   1863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1864\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcond_samples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1865\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcond_samples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1866\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "              ('news', word) for word in brown.words(categories='news') if word.islower())\n",
    "print(cfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculateCFD(cfdconditions, cfdevents):\n",
    "    # Write your code here\n",
    "  \n",
    "    stopword=stopwords.words('english')\n",
    "    t=([[genre, word.lower()] for genre in cfdconditions for word in brown.words (categories=genre)         if word.lower() not in stopword])          \n",
    "    cdev_cfd=nltk.ConditionalFreqDist(t)\n",
    "    cdev_cfd.tabulate(conditions=cfdconditions,samples=cfdevents)\n",
    "    tlist=[]\n",
    "    for i in t:\n",
    "        if i[1].endswith('ing'):\n",
    "            tlist.append((i[0],'ing'))\n",
    "\n",
    "        elif i[1].endswith('ed'):\n",
    "            tlist.append((i[0],'ed'))\n",
    "\n",
    "    inged_cfd=nltk.ConditionalFreqDist(tlist)      \n",
    "    inged_cfd.tabulate(conditions=cfdconditions,samples=['ed','ing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cfd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-fcb8c9986fae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreuters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreuters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'zinc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cfd' is not defined"
     ]
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist([ \n",
    "(genre, word) \n",
    "for genre in brown.categories() \n",
    "for word in brown.words(categories=genre) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      tonnes   year \n",
      "sugar    355    196 \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist([ \n",
    "(genre, word) \n",
    "for genre in reuters.categories() \n",
    "for word in reuters.words(categories=genre) ])\n",
    "#print(cfd.conditions())\n",
    "print(cfd.tabulate(conditions=['sugar'], samples=['tonnes','year']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    textURL='https://hrcdn.net/s3_pub/istreet-assets/2KDELtu3svGwJgNXUXFE7Q/001.txt'\n",
    "    textcontent = request.urlopen(textURL).read().decode('utf-8')\n",
    "    w = nltk.word_tokenize(textcontent)\n",
    "    tokenizedlcwords = [x.lower() for x in w]\n",
    "    noofwords=len(tokenizedlcwords)\n",
    "    noofunqwords = len(set(tokenizedlcwords))\n",
    "    wordcov = noofwords/noofunqwords\n",
    "    wordfreq = nltk.FreqDist([x for x in tokenizedlcwords if x.isalpha()])\n",
    "    top = wordfreq.most_common(1)\n",
    "    maxfreq=top[0][1]\n",
    "    return noofwords, noofunqwords, wordcov, maxfreq  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' is', ' cool']\n"
     ]
    }
   ],
   "source": [
    "s = 'Python is cool!!!'\n",
    "print(re.findall(r'\\s\\w+\\b', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['G', 'A', 'C', 'L', 'T', 'B', 'T', 'B', 'T', 'T', 'R', 'W', 'I', '17', 'C', 'A', 'T', 'T', 'E', '2002', 'T', 'N', 'I', 'T', 'A', 'W', 'W', 'H', 'M', '1960', 'H', '1980', 'W']\n"
     ]
    }
   ],
   "source": [
    "    # Write your code here\n",
    "    import nltk\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    pat = r'[A-Za-z0-9_]+'\n",
    "    stopword=stopwords.words('english')\n",
    "    words = nltk.regexp_tokenize(textcontent, pat)\n",
    "    tokenizedwords = [w.lower() for w in words]\n",
    "    #print(tokenizedwords)\n",
    "    tokenizedwordsbigrams = list(nltk.bigrams(tokenizedwords))\n",
    "    #print(tokenizedwordsbigrams)\n",
    "    tokenizednonstopwordsbigrams = [ (w1, w2) for w1, w2 in tokenizedwordsbigrams if w1 not in stopword     and w2 not in stopword ]\n",
    "    #print(tokenizednonstopwordsbigrams)\n",
    "    cfd_bigrams = nltk.ConditionalFreqDist(tokenizednonstopwordsbigrams)\n",
    "    #print(cfd_bigrams)\n",
    "    mostfrequentwordafter = cfd_bigrams[word].most_common(3)\n",
    "    #print(mostfrequentwordafter)\n",
    "    gen_text = nltk.Text(tokenizedwords)\n",
    "    #print(gen_text)\n",
    "    collocationwords = gen_text.collocations()\n",
    "    print(collocationwords)\n",
    "    #return mostfrequentwordafter, collocationwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-74c63fed7603>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenesis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english-kjv.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgen_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgen_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollocations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\text.py\u001b[0m in \u001b[0;36mcollocations\u001b[1;34m(self, num, window_size)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m         collocation_strings = [\n\u001b[1;32m--> 444\u001b[1;33m             \u001b[0mw1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mw2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollocation_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m         ]\n\u001b[0;32m    446\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollocation_strings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"; \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\text.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m         collocation_strings = [\n\u001b[1;32m--> 444\u001b[1;33m             \u001b[0mw1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mw2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollocation_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m         ]\n\u001b[0;32m    446\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollocation_strings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"; \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import genesis\n",
    "tokens = genesis.words('english-kjv.txt')\n",
    "gen_text = nltk.Text(tokens)\n",
    "col=gen_text.collocations()\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('GUARD', 21), ('GUARDS', 8)]\n",
      "[(('ARTHUR', ':'), 217), ((\"'\", 's'), 140), ((']', '['), 94), (('!', '['), 82), ((':', 'Oh'), 82), (('Oh', ','), 79), ((\"'\", 't'), 77), (('LAUNCELOT', ':'), 76), (('1', ':'), 75), (('#', '1'), 75), (('.', 'ARTHUR'), 75), (('GALAHAD', ':'), 69), ((':', '['), 67), ((':', 'I'), 65), (('!', 'ARTHUR'), 63), (('FATHER', ':'), 63), (('BEDEVERE', ':'), 61), (('KNIGHT', ':'), 59), ((',', 'I'), 58), ((':', 'No'), 56), ((':', 'What'), 55), ((':', 'Well'), 52), (('I', \"'\"), 51), (('Well', ','), 50), (('VILLAGER', '#'), 47), ((\"'\", 're'), 41), (('#', '2'), 41), (('2', ':'), 40), (('of', 'the'), 39), (('ROBIN', ':'), 39), (('.', 'I'), 38), ((':', 'Yes'), 38), (('No', ','), 38), (('.', '['), 38), ((',', 'you'), 36), (('Ni', '!'), 36), (('boom', ']'), 35), (('?', 'ARTHUR'), 34), ((\"'\", 'm'), 34), (('[', 'boom'), 34), (('BLACK', 'KNIGHT'), 32), (('GUARD', '#'), 32), (('witch', '!'), 31), ((':', 'You'), 30), (('it', '!'), 30), ((',', 'and'), 30), ((',', 'no'), 30), (('[', 'singing'), 29), (('HEAD', 'KNIGHT'), 29), (('TIM', ':'), 28), ((':', 'And'), 27), (('.', 'GALAHAD'), 27), (('.', 'LAUNCELOT'), 27), (('clop', 'clop'), 26), (('don', \"'\"), 26), (('singing', ']'), 26), (('HERBERT', ':'), 26), (('!', 'A'), 25), (('HEAD', ':'), 25), (('Hello', '.'), 25)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "eng_bigrams = nltk.bigrams(text6)\n",
    "#eng_cfd = nltk.ConditionalFreqDist(eng_bigrams)\n",
    "#text1_freq = nltk.FreqDist(text6)\n",
    "print(eng_cfd['FRENCH'].most_common(2))\n",
    "print(nltk.FreqDist(eng_bigrams).most_common(60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performStemAndLemma(textcontent):\n",
    "    # Write your code here\n",
    "    import nltk\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk import PorterStemmer\n",
    "    from nltk import LancasterStemmer\n",
    "    pat = r'[A-Za-z0-9_]+'\n",
    "    stopword=stopwords.words('english')\n",
    "    tokenizedwords = nltk.regexp_tokenize(textcontent, pat)\n",
    "    tokenizedwords = [w.lower() for w in set(tokenizedwords)]\n",
    "    filteredwords = [w for w in tokenizedwords if w not in stopword]\n",
    "    porter = PorterStemmer()\n",
    "    porterstemmedwords = [porter.stem(word) for word in filteredwords ]\n",
    "    lancaster = LancasterStemmer()\n",
    "    lancasterstemmedwords = [lancaster.stem(word) for word in filteredwords ]\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    lemmatizedwords = [wnl.lemmatize(word) for word in filteredwords ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "print(wnl.lemmatize('women'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pow\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print(lancaster.stem('power'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagPOS(textcontent, taggedtextcontent, defined_tags):\n",
    "    # Write your code here\n",
    "    import nltk\n",
    "    words = nltk.word_tokenize(textcontent)\n",
    "    nltk_pos_tags = nltk.pos_tag(words)\n",
    "    tagged_pos_tag = [ nltk.tag.str2tuple(word) for word in taggedtextcontent.split() ]\n",
    "    baseline_tagger = nltk.UnigramTagger(model=defined_tags)\n",
    "    unigram_pos_tag = baseline_tagger.tag(words)\n",
    "    return nltk_pos_tags, tagged_pos_tag, unigram_pos_tag\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fly', 'NN')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tagged_token = nltk.tag.str2tuple('fly/NN')\n",
    "print(tagged_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    }
   ],
   "source": [
    "a=len(text6)\n",
    "b=len(set(text6))\n",
    "#print(a/b)\n",
    "w=[w for w in text6 if w.isdigit()]\n",
    "print(len(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    }
   ],
   "source": [
    "sun_words = [word for word in text6 if word.endswith('ly') ]\n",
    "print(len(sun_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'is', 'cool']\n"
     ]
    }
   ],
   "source": [
    "pat = r'\\w+'\n",
    "print(nltk.regexp_tokenize('Python is cool!!!', pat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ceremoni\n",
      "['Python', 'is', 'cool', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "porter = nltk.PorterStemmer()\n",
    "print(porter.stem('ceremony'))\n",
    "print(nltk.word_tokenize('Python is cool!!!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lying\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print(lancaster.stem('lying'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'you', 'can', 'not', 'do', 'great', 'things', ',', 'do', 'small', 'things', 'in', 'a', 'great', 'way']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize('if you cannot do great things, do small things in a great way'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('https://inclass.kaggle.com/c/si650winter11/download/training.txt and load it to the variable ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'joined'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-27ea6260a7c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misPalindrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"liril\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"liril\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0ms1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoined\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreveresed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"yes\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'joined'"
     ]
    }
   ],
   "source": [
    "def isPalindrome(S):\n",
    "    return S==S[::-1]\n",
    "print(isPalindrome(\"liril\"))\n",
    "s=\"liril\"\n",
    "s1=\"\".joined(s.reveresed())\n",
    "if s==s1:\n",
    "    print(\"yes\")\n",
    "\n",
    "#for i in \"liril\":\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def fibonacci(n):\n",
    "    a=0\n",
    "    b=1\n",
    "    l=[a,b]\n",
    "    i=0\n",
    "    if n > 2:\n",
    "        while i < n-2:    \n",
    "            c=a+b\n",
    "            a=b\n",
    "            b=c\n",
    "            i+=1\n",
    "            l.append(c)\n",
    "    print(l)\n",
    "#n=10\n",
    "fibonacci(3)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
